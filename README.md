# stolen-attention
This repository contains the code used to create the results in my Master's Thesis, [Stolen Attention in Transformers](). Included in `modules/` is a [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/) implementation of a transformer that is lightweight and easy to understand, making it great for tinkering and research. Main training and validation scripts are in the root directory. There, the `trainer` module makes it easy to scale to multiple GPUs or use automatic mixed-precision; I employed both strategies to improve training times. Finally, preprocessing scripts live in the `scripts/` folder.

## Abstract
In the field of Natural Language Processing (NLP), today's best models rely on the construction of an embedding space. Upon input to a language model, tokens are translated into dense, high-dimensional vectors. Over the process of training, tokens' embeddings are organized for spatial-relatedness. This allows models to greatly generalize, as the meaning of a word can be expressed in terms of neighboring embeddings. 

A language model has a fixed vocabulary, whose embeddings are expressed using weight matrices. The matrix at the input of the network translates a token to its internal representation. As it is shown a sequence, the language model builds a representation of its prediction vector $h_t$. Then at the output, the prediction vector is compared against embeddings in the output matrix to select the next word. This is traditionally done with dot-product softmax. 

Unfortunately, certain tokens have reduced potential to receive probability mass under dot-product softmax. Embeddings who are interior to the convex hull of the output weight matrix cannot be assigned high probability. While this effect has been previously studied in other language models, the transformer architecture also uses dot-product softmax in another location: the attention mechanism. In this work, we demonstrate the Stolen Attention Effect, where certain embeddings cannot be assigned high attention weights. Further, we suggest an architectural change that may overcome the effect. By using a variant of Euclidean distance instead of dot-product in the attention mechanism, we yield a 14.27\% improvement on OpenBookQA. 
